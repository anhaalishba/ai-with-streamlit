AI Code Reviewer
ğŸ§  Overview

AI Code Reviewer is a multi-model AI-powered application built with LangChain and Streamlit.
It automatically reviews code, provides feedback, refactors it for better performance, and shares improvement tips â€” all in seconds!

This project demonstrates how to use three different AI models from three platforms:

ğŸ§© Gemini (Google) â†’ For merging final professional reports

ğŸ¤— Hugging Face (distilgpt2) â†’ For generating smart coding tips

ğŸ¦™ Open Source LLaMA (via Together/OpenAI endpoint) â†’ For deep code review and refactoring

ğŸš€ Features

âœ… Multi-model integration (Gemini + Hugging Face + LLaMA)
âœ… Parallel processing using RunnableParallel
âœ… Automatic bug detection & code optimization suggestions
âœ… Refactored version of the same code
âœ… Professional, combined report generation
âœ… Simple, interactive Streamlit UI

ğŸ—ï¸ Tech Stack

Python

LangChain

Streamlit

Google Gemini

Hugging Face Transformers

LLaMA Model (Open Source)

dotenv for environment management

âš™ï¸ Installation Steps
1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/ai-code-reviewer.git
cd ai-code-reviewer

2ï¸âƒ£ Create a Virtual Environment
python -m venv venv
venv\Scripts\activate    # (on Windows)
# or
source venv/bin/activate  # (on Mac/Linux)

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Create a .env File

Inside your project folder, create a file named .env and add:

OPENAI_API_KEY=your_openai_or_together_api_key
OPENAI_BASE_URL=your_model_endpoint_url

5ï¸âƒ£ Run the App
streamlit run app.py

ğŸ§© Project Structure
ğŸ“ ai-code-reviewer/
â”‚
â”œâ”€â”€ app.py               # Main Streamlit app
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ README.md            # Documentation
â””â”€â”€ .env                 # API keys (not shared publicly)

ğŸ“„ Example Output

When you paste your code and click "Review Code", the app generates:

Review Section â†’ Highlights bugs, logic issues, and improvements

Refactored Code â†’ Cleaner, optimized version

Tips Section â†’ 3 best practices generated by Hugging Face

Merged Report â†’ A final formatted report from Gemini

ğŸ§  Concepts Demonstrated

Multi-model orchestration with LangChain

Using RunnableParallel for parallel AI calls

Combining results with a merge chain

PromptTemplate usage per model

Streamlit UI integration

âš¡ RunnableParallel â€“ How It Works

RunnableParallel allows multiple models or chains to run simultaneously rather than sequentially.
In this project, it helps the app generate:

Review (using LLaMA model)

Refactor (using LLaMA model)

Tips (using Hugging Face model)

â€” all at the same time!

ğŸ”§ Code Example
parallel_chain = RunnableParallel({
    "review": review_template | code_review_model | parser,
    "refactor": refactor_template | code_review_model | parser,
    "tips": tips_template | suggestions_model | parser
})


This chain executes three different tasks concurrently, collects all results, and passes them to Gemini, which merges them into a single, professional Code Review Report.

ğŸ§© Benefits

Faster execution (parallel processing)

Efficient use of multiple models

Demonstrates advanced LangChain pipeline design

âœ¨ Future Improvements

Add syntax highlighting for reviewed code

Support multiple languages (Python, JavaScript, etc.)

Export reports as PDF

ğŸ‘©â€ğŸ’» Author

Developed by: Anha Alishba
